# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xv6ssGQ7FREmeKX5Lt9KlHykeB4604nc
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt 
import seaborn as sns
import scipy
from scipy.stats import norm 
import plotly.offline as py
py.init_notebook_mode(connected=True)
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.offline as offline
offline.init_notebook_mode()

from google.colab import files # For colab only
files.upload()

#Comment out if not using Colab. This function is used to solve plotting problem of plotly.offline in colab.
#'''
def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))
#'''

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!ls -l ~/.kaggle
!cat ~/.kaggle/kaggle.json

!pip install -q kaggle
!pip install -q kaggle-cli

!kaggle datasets list

!kaggle competitions download -c home-credit-default-risk

!ls

import zipfile

!unzip application_test.csv.zip
!unzip application_train.csv.zip
!unzip bureau_balance.csv.zip
!unzip bureau.csv.zip
!unzip credit_card_balance.csv.zip
!unzip installments_payments.csv.zip
!unzip POS_CASH_balance.csv.zip
!unzip previous_application.csv.zip
!unzip sample_submission.csv.zip

app_test = pd.read_csv('application_test.csv')
app_train = pd.read_csv('application_train.csv')
bureau_bal = pd.read_csv('bureau_balance.csv')
bureau = pd.read_csv('bureau.csv')
card_bal = pd.read_csv('credit_card_balance.csv')
install_pay = pd.read_csv('installments_payments.csv')
prev_app = pd.read_csv('previous_application.csv')
sample_sub = pd.read_csv('sample_submission.csv')
cash_bal = pd.read_csv('POS_CASH_balance.csv')

app_train.columns

#finding missing value of train dataset
total_null=app_train.isnull().sum().sort_values(ascending=False)
percentage=(app_train.isnull().sum()/app_train.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of Cash Balance dataset
total_null=cash_bal.isnull().sum().sort_values(ascending=False)
percentage=(cash_bal.isnull().sum()/cash_bal.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of Bureau dataset
total_null=bureau.isnull().sum().sort_values(ascending=False)
percentage=(bureau.isnull().sum()/bureau.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of Bureau Balance dataset
total_null=bureau_bal.isnull().sum().sort_values(ascending=False)
percentage=(bureau_bal.isnull().sum()/bureau_bal.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of Previous application dataset
total_null=prev_app.isnull().sum().sort_values(ascending=False)
percentage=(prev_app.isnull().sum()/prev_app.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of Install Payment dataset
total_null=install_pay.isnull().sum().sort_values(ascending=False)
percentage=(install_pay.isnull().sum()/install_pay.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

#finding missing value of application test dataset
total_null=app_test.isnull().sum().sort_values(ascending=False)
percentage=(app_test.isnull().sum()/app_test.isnull().count() *100).sort_values(ascending=False)
missing_train_data=pd.concat([total_null,percentage],axis=1,keys=["Total_null","Percentage"])

missing_train_data.head()

plt.figure(figsize=(10,5),dpi=80)
plt.hist(app_train['TARGET'],bins=50);

a = len(app_train['TARGET'])
x,count_0 = app_train.TARGET.value_counts()

print(str(round(count_0*100/a,4))+'% are defaulters')

temp=app_train['NAME_CONTRACT_TYPE'].value_counts()
x=temp.index
y=temp.values
plt.pie(x=temp.values,explode=(0.1,0),labels=temp.index,startangle=80,autopct='%1.1f%%',
        colors=['#F1BF1B','#B1F11B'],frame=False,radius=1.5)

temp = app_train['NAME_INCOME_TYPE'].value_counts()

temp_y0 = []
temp_y1=[]

for val in temp.index:
  temp_y1.append(np.sum(app_train["TARGET"][app_train["NAME_INCOME_TYPE"]==val] == 1))
  temp_y0.append(np.sum(app_train["TARGET"][app_train["NAME_INCOME_TYPE"]==val] == 1))
  
trace1 = go.Bar(x= temp.index, y = (temp_y1 / temp.sum()) * 100,name='YES') #go is plotly.graph_objs
trace2 = go.Bar(x= temp.index, y = (temp_y0 / temp.sum()) * 100,name='NO') #go is plotly.graph_objs

data = [trace1, trace2]
layout = go.Layout(
    title = "Income sources of Applicant's in terms of loan is repayed or not  in %",
    #barmode='stack',
    width = 1000,
    xaxis=dict(
        title='Income source',
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
    ),
    yaxis=dict(
        title='Count in %',
        titlefont=dict(
            size=16,
            color='rgb(107, 107, 107)'
        ),
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
)
)

fig = go.Figure(data=data, layout=layout)
configure_plotly_browser_state()  #For plotly offline in Colab
iplot(fig)

temp = app_train['OCCUPATION_TYPE'].value_counts()

temp_y0 = []
temp_y1=[]

for val in temp.index:
  temp_y1.append(np.sum(app_train["TARGET"][app_train["OCCUPATION_TYPE"]==val] == 1))
  temp_y0.append(np.sum(app_train["TARGET"][app_train["OCCUPATION_TYPE"]==val] == 1))
  
trace1 = go.Bar(x= temp.index, y = (temp_y1 / temp.sum()) * 100,name='YES') #go is plotly.graph_objs
trace2 = go.Bar(x= temp.index, y = (temp_y0 / temp.sum()) * 100,name='NO') #go is plotly.graph_objs

data = [trace1, trace2]
layout = go.Layout(
    title = "Occupation of Applicant's in terms of loan is repayed or not  in %",
    #barmode='stack',
    width = 1000,
    xaxis=dict(
        title='Applicants Occupation' ,
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
    ),
    yaxis=dict(
        title='Count in %',
        titlefont=dict(
            size=16,
            color='rgb(107, 107, 107)'
        ),
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
)
)

fig = go.Figure(data=data, layout=layout)
configure_plotly_browser_state()  #For plotly offline in Colab
iplot(fig)

prev=prev_app['NAME_CLIENT_TYPE'].value_counts()
trace = go.Pie(labels=prev.index, values=prev.values)
configure_plotly_browser_state() #Comment out if not using Colab. This function is used to solve plotting problem of plotly.offline in colab.
py.iplot([trace], filename='basic_pie_chart')

prev=prev_app['NAME_CONTRACT_TYPE'].value_counts()
trace = go.Pie(labels=prev.index, values=prev.values)
configure_plotly_browser_state() #Comment out if not using Colab. This function is used to solve plotting problem of plotly.offline in colab.
py.iplot([trace], filename='basic_pie_chart')

prev=prev_app['CHANNEL_TYPE'].value_counts()
trace = go.Pie(labels=prev.index, values=prev.values)
configure_plotly_browser_state()
py.iplot([trace], filename='basic_pie_chart')

from sklearn.preprocessing import LabelEncoder
lbl=LabelEncoder()
lbl_count=0
for i in app_train:
    if app_train[i].dtype=='object':
        if len(list(app_train[i].unique())) <= 2:
            lbl.fit(app_train[i])
            #through this code we encode only those column who have less 
            #than or equal to 2 categorical variable
            app_train[i]=lbl.transform(app_train[i])
            app_test[i]=lbl.transform(app_test[i])
            lbl_count +=1
print('%d column were encoded.'%lbl_count)

#for one hot enocder we use pd.get_dummies
app_train=pd.get_dummies(app_train)
app_test=pd.get_dummies(app_test)
print("application_train feature shape:",app_train.shape)
print("application_test feature shape:",app_test.shape)

train_target=app_train['TARGET']
app_train,app_test=app_train.align(app_test,axis=1,join='inner')

app_train['TARGET']=train_target

(app_train['DAYS_BIRTH']/(-365)).describe()

app_train.isnull().sum()

prev_category = pd.get_dummies(prev_app)
bureau_category = pd.get_dummies(bureau)
pos_category = pd.get_dummies(cash_bal)
credit_category= pd.get_dummies(card_bal)

app_train=app_train.fillna(0)
app_test=app_test.fillna(0)

from sklearn.model_selection import train_test_split 
import lightgbm as lgb

app_test['is_test'] = 1 
app_test['is_train'] = 0
app_train['is_test'] = 0
app_train['is_train'] = 1

# target variable
Y = app_train['TARGET']
train_X = app_train.drop(['TARGET'], axis = 1)

# test ID
test_id = app_train['SK_ID_CURR']
test_X = app_test

# merge train and test datasets for preprocessing
data = pd.concat([train_X, test_X], axis=0)

prev_apps = prev_app[['SK_ID_PREV','SK_ID_CURR']].groupby('SK_ID_CURR').count()

prev_app['SK_ID_PREV'] = prev_app['SK_ID_CURR'].map(prev_apps['SK_ID_PREV'])

prev_apps_avg = prev_apps.groupby('SK_ID_CURR').mean()
prev_apps_avg.columns = ['p ' + col for col in prev_apps_avg.columns]
data = data.merge(right = prev_apps_avg.reset_index(), how='left', on = 'SK_ID_CURR')

bureau_avg = bureau.groupby('SK_ID_CURR').mean()
bureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']
bureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]
data = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')

install_paying= install_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()
install_pay['SK_ID_PREV'] = install_pay['SK_ID_CURR'].map(install_paying['SK_ID_PREV'])

## Average values for all other variables in installments payments
avg_inst = install_pay.groupby('SK_ID_CURR').mean()
avg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]
data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')

pos_cash = cash_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()
cash_bal['SK_ID_PREV'] = cash_bal['SK_ID_CURR'].map(pos_cash['SK_ID_PREV'])

## Average Values for all other variables in pos cash
POS_avg = cash_bal.groupby('SK_ID_CURR').mean()
data = data.merge(right=POS_avg.reset_index(), how='left', on='SK_ID_CURR')

credit_balns= card_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()
card_bal['SK_ID_PREV'] = card_bal['SK_ID_CURR'].map(credit_balns['SK_ID_PREV'])

### average of all other columns 
avg_credit_bal = card_bal.groupby('SK_ID_CURR').mean()
avg_credit_bal.columns = ['credit_bal_' + f_ for f_ in avg_credit_bal.columns]
data = data.merge(right=avg_credit_bal.reset_index(), how='left', on='SK_ID_CURR')

#final training and testing data
ignore_features = ['SK_ID_CURR', 'is_train', 'is_test']
relevant_features = [col for col in data.columns if col not in ignore_features]
trainX = data[data['is_train'] == 1][relevant_features]
testX = data[data['is_test'] == 1][relevant_features]

x_train, x_val, y_train, y_val = train_test_split(trainX, Y, test_size=0.2, random_state=18)
lgb_train = lgb.Dataset(data=x_train, label=y_train)
lgb_eval = lgb.Dataset(data=x_val, label=y_val)

import lightgbm as lgb
import lightgbm as lgb
params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 
          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,
          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, 
          'min_split_gain':.01, 'min_child_weight':1}
model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=170, verbose_eval=200)

import lightgbm as lgb
params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 
          'learning_rate': 0.00875, 'num_leaves': 50, 'num_iteration': 5000, 'verbose': 0 ,
          'colsample_bytree':.8, 'subsample':.9, 'max_depth':8, 'reg_alpha':.1, 'reg_lambda':.1, 
          'min_split_gain':.01, 'min_child_weight':1}
model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=170, verbose_eval=200)

lgb.plot_importance(model, max_num_features=100, figsize=(15, 30),color="red")

preds = model.predict(testX)
sub = app_test[['SK_ID_CURR']].copy()
sub['TARGET'] = preds
sub.to_csv('sub.csv', index= False)
sub.head(10)

files.download('sub.csv')

sub.head()

